{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d6cd332",
   "metadata": {},
   "source": [
    "# Preprocess EYATH Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24dda2c",
   "metadata": {},
   "source": [
    "**!!! IMORTANT !!!**<br>\n",
    "This script is meant to be used iteratively and help with the MANUAL cleaning of the Excel files and normalization to a common format. After this is done, the conversion to .csv files (ready to be analysed) is performed automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae46724",
   "metadata": {},
   "source": [
    "The format of the final table (if all columns are present is the following) - May be modified in the future\n",
    "| N   | E   | Time (Greek am) | Temp (°C) | Turbidity (NTU) | Chla (mg m⁻³) | Date       |\n",
    "|-----|-----|----------------|-----------|-----------------|---------------|------------|\n",
    "|     |     |                |           |                 |               |            |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb598c7",
   "metadata": {},
   "source": [
    "EYATH In situ measurements so far have been given in Excel format following a similar structure.\n",
    "\n",
    "<br> **Sampling method**:\n",
    "\n",
    "- Samples collected from side of vessel (in the middle) at a 20cm distance\n",
    "- Clean vessel\n",
    "- Sampling at point with no sediment influence\n",
    "- Turbidity measurements (might contain surface organic/inorganic particles transferred by the river or air but not the same for Chl-a measurements, as it is removed due to the methodology used\n",
    "- Measurements from the first 10-15cm of the water column\n",
    "\n",
    "<br> **Additional observations by EYATH for samples of 26/06/2025**:\n",
    "\n",
    "- No waves\n",
    "- No rain previous days\n",
    "- No macrophytes or vegetation at the sampling stations\n",
    "- High transparency in the water column\n",
    "- Dominant algae species with low Chl-a content\n",
    "\n",
    "<br>**Details on specific files**:\n",
    "<br>\n",
    "- `HELOISA_EYATH 2025-2026.xlsx` [Polyfytos Reservoir, 26/06/2025] -> `HELOISA_EYATH 2025-2026_cleaned.xlsx (the cleaned version)`<br>\n",
    "*A collection of 30 samples were collected at surface level with wide spatial coverage and measuring Chl-a and turbidity levels. Water temperature was also measured.*\n",
    "<br><br> **Excel formatting issues**:\n",
    "    - N/E column names needed to be changed to latin\n",
    "    - row 4 of E column and rows 3, 13, 14 and 18 in column Time (Greek am) had extra spaces\n",
    "- `Data_Polyphytos_21-22.xlsx` [Polyfytos Reservoir, 04/10/2021, 20/01/2022, 17/06/2022, 30/09/2022] -> `Data_Polyphytos_21-22_cleaned.xlsx (the cleaned version)`<br>\n",
    "*A collection of 30 samples per day (except for 20/01/2022) were collected at surface level with wide spatial coverage and measuring Chl-a and turbidity levels. Water temperature was also measured.*\n",
    "<br><br> **Excel formatting issues**:\n",
    "    - 20.01.2022 sheet did not follow the format of the rest, but no satellite overpass occured that day, so it was discarded\n",
    "    - the 04.10.2021 was dropped as it did not coincide with satellite overpass (also it did not follow the general format) \n",
    "    - Temperature was only present in 17.06.2022. Was renamed to \"Temp oC\" to match the general format\n",
    "\n",
    "<br>**Depth (m) column**:\n",
    "The Depth (m) column is not retained for the final .csv, as it does not represent the sampling depth. If new information arises modifications will be performed accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348ab8d1",
   "metadata": {},
   "source": [
    "Here you can choose the file you will preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f7205c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the file here\n",
    "file = \"Data_Polyphytos_21-22_cleaned.xlsx\" # \"HELOISA_EYATH 2025-2026_cleaned.xlsx\", \"Data_Polyphytos_21-22_cleaned.xlsx\"\n",
    "\n",
    "insitu = \"../data/xlsx/\" + file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d5dcf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization to follow Degrees Decimal Minutes (DMM) format\n",
    "def clean_coordinate(coord,direction):\n",
    "    d,m,md = coord.split(\" \")\n",
    "\n",
    "    return str(int(d)) + \" \" + m + \".\" + md + \" \" + direction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78b92a5",
   "metadata": {},
   "source": [
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d159ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b907bd82",
   "metadata": {},
   "source": [
    "Some helper function that help with the validation of the Excel file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a4d3170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate column with regex\n",
    "def validate_column(df: pd.DataFrame, col: str, pattern: str):\n",
    "    # Check validity of column\n",
    "    df[\"is_valid\"] = df[col].astype(str).str.match(pattern)\n",
    "\n",
    "    # Filter wrong rows\n",
    "    invalid_rows = df[~df[\"is_valid\"]].drop(columns=\"is_valid\")\n",
    "\n",
    "    # Overall pass/fail\n",
    "    overall = \"✅\" if df[\"is_valid\"].all() else \"❌\"\n",
    "\n",
    "    print(f\"Overall validation of column {col}:\", overall)\n",
    "    if not df[\"is_valid\"].all():\n",
    "        print(\"\\nNon-conforming rows:\\n\", invalid_rows[col], \"\\n\")\n",
    "    \n",
    "    return overall\n",
    "\n",
    "# Check if all columns are present\n",
    "def check_columns(df: pd.DataFrame, required: list[str], secondary_columns:bool=True) -> bool:\n",
    "    missing = [col for col in required if col not in df.columns]\n",
    "    if missing:\n",
    "        if not secondary_columns:\n",
    "            print(\"❌ There are missing columns. Either modify the file or the code based on your requirements.\\n\")\n",
    "        else:\n",
    "            print(\"⚠️ There are missing columns from the full list of expected ones. These columns are considered secondary, so you are juts given a warning.\")\n",
    "        print(\"Missing columns:\", missing)\n",
    "        return False, missing\n",
    "    else:\n",
    "        print(\"✅ All required columns are present.\\n\")\n",
    "    return True, missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61db2557",
   "metadata": {},
   "source": [
    "### This cell helps the user to ensure the Excel follows the desired format (and correct it MANUALLY if needed until it conforms) and save it in a uniform csv format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ba8fce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheet Name: 17.06.2022\n",
      "Index(['N', 'E', 'Time (Greek am)', 'Temp oC', 'Turbidity (NTU)',\n",
      "       'Chla (mg m-3)', 'Depth (m)'],\n",
      "      dtype='object')\n",
      "✅ All required columns are present.\n",
      "\n",
      "✅ All required columns are present.\n",
      "\n",
      "Keys after removing secondary missing keys: dict_keys(['N', 'E', 'Time (Greek am)', 'Temp oC', 'Turbidity (NTU)', 'Chla (mg m-3)', 'Depth (m)'])\n",
      "Overall validation of column N: ✅\n",
      "Overall validation of column E: ✅\n",
      "Overall validation of column Time (Greek am): ✅\n",
      "Overall validation of column Temp oC: ✅\n",
      "Overall validation of column Turbidity (NTU): ✅\n",
      "Overall validation of column Chla (mg m-3): ✅\n",
      "Overall validation of column Depth (m): ✅\n",
      "✅ ALL VALIDATION TESTS PASSED..........\n",
      "Normalizing coordinates...\n",
      "Saving converted .csv\n",
      "Sheet Name: 30.09.2022\n",
      "Index(['N', 'E', 'Time (Greek am)', 'Turbidity (NTU)', 'Chla (mg m-3)',\n",
      "       'Depth (m)'],\n",
      "      dtype='object')\n",
      "✅ All required columns are present.\n",
      "\n",
      "⚠️ There are missing columns from the full list of expected ones. These columns are considered secondary, so you are juts given a warning.\n",
      "Missing columns: ['Temp oC']\n",
      "Keys after removing secondary missing keys: dict_keys(['N', 'E', 'Time (Greek am)', 'Turbidity (NTU)', 'Chla (mg m-3)', 'Depth (m)'])\n",
      "Overall validation of column N: ✅\n",
      "Overall validation of column E: ✅\n",
      "Overall validation of column Time (Greek am): ✅\n",
      "Overall validation of column Turbidity (NTU): ✅\n",
      "Overall validation of column Chla (mg m-3): ✅\n",
      "Overall validation of column Depth (m): ✅\n",
      "✅ ALL VALIDATION TESTS PASSED..........\n",
      "Normalizing coordinates...\n",
      "Saving converted .csv\n"
     ]
    }
   ],
   "source": [
    "file = pd.ExcelFile(insitu)  \n",
    "\n",
    "# For each sheet (=date)\n",
    "for sheetName in file.sheet_names:\n",
    "    print(f\"Sheet Name: {sheetName}\")\n",
    "    # Read in situ data for the day\n",
    "    df = pd.read_excel(insitu,sheet_name=sheetName,index_col=False)\n",
    "\n",
    "    \n",
    "    # Removing unnamed columns using drop function\n",
    "    df.drop(df.columns[df.columns.str.contains(\n",
    "        'unnamed', case=False)], axis=1, inplace=True)\n",
    "\n",
    "    print(df.columns)\n",
    "\n",
    "    # Validate sheet data\n",
    "    val_dic = {\"N\": r\"^\\d{2,3} \\d{2,3} \\d{2,3}$\",\n",
    "               \"E\": r\"^\\d{2,3} \\d{2,3} \\d{2,3}$\",\n",
    "               \"Time (Greek am)\": r\"^\\d{1,2} \\d{2}$\",\n",
    "               \"Temp oC\": r\"^[+-]?\\d+(\\.\\d+)?$\",\n",
    "               \"Turbidity (NTU)\": r\"^[+-]?\\d+(\\.\\d+)?$\",\n",
    "               \"Chla (mg m-3)\": r\"^[+-]?\\d+(\\.\\d+)?$\",\n",
    "               \"Depth (m)\": r\"^[+-]?\\d+(\\.\\d+)?$\"}\n",
    "    \n",
    "    # Check if all columns are present\n",
    "    excluded_keys = {'Temp oC', 'Depth (m)'}\n",
    "    if not check_columns(df, val_dic.keys() - excluded_keys)[0]:\n",
    "        exit\n",
    "    _ , missing = check_columns(df, excluded_keys, secondary_columns=True)\n",
    "    for m in missing:\n",
    "        val_dic.pop(m, None)\n",
    "    print(f\"Keys after removing secondary missing keys: {val_dic.keys()}\")\n",
    "\n",
    "    # Validate format of columns\n",
    "    atleast_one = True\n",
    "    for c, pattern in val_dic.items():\n",
    "        if not validate_column(df=df, col=c, pattern=pattern):\n",
    "            atleast_one = False\n",
    "    if not atleast_one:\n",
    "        exit\n",
    "    else:\n",
    "        print(\"✅ ALL VALIDATION TESTS PASSED..........\")\n",
    "        df = df.drop(columns=['is_valid', 'Depth (m)'], errors='ignore')\n",
    "\n",
    "    # Normalize excel to common csv format\n",
    "    try:\n",
    "        if(re.match(val_dic.get('N'), str(df['N'].iloc[0]))):\n",
    "            # DMM\n",
    "            print(\"Normalizing coordinates...\")\n",
    "            df['N']=df['N'].apply(lambda x:clean_coordinate(x,'N'))\n",
    "            df['E']=df['E'].apply(lambda x:clean_coordinate(x,'E'))\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        # Store date\n",
    "        df['date']=pd.to_datetime(sheetName,dayfirst=True)\n",
    "        # export to csv\n",
    "        date_title =  datetime.strptime(sheetName.replace(\" \", \"\"), \"%d.%m.%Y\").strftime(\"%Y%m%d\")\n",
    "        print(\"Saving converted .csv\")\n",
    "        df.to_csv(f'../data/csv/Data_Polyphytos_{date_title}.csv',index=False)\n",
    "        \n",
    "    except Exception as e:    \n",
    "        print(e)\n",
    "        print(f'Sheet Name : {sheetName} could not be processed')\n",
    "        continue\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hydroscan_notebook-",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
